{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df53fb6e-019e-4f42-a363-8e1f869346c5",
   "metadata": {},
   "source": [
    "# Generative Pre-trained Transformer\n",
    "\n",
    "## Problem 4: Self-attention\n",
    "\n",
    "In this exercise we train a small GPT model to generate text as written by William Shakespeare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08705f-066a-452a-b7e9-383fe4c15305",
   "metadata": {},
   "source": [
    "### Import Python modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d86e73-6716-4e76-aafc-212dcbadf7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524836f0-29ad-4bfd-a839-83c2f6ecef1a",
   "metadata": {},
   "source": [
    "### Initialize parameters for neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb82b0-b94c-411e-8612-83ac930070f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32      # how many independent sequences will we process in parallel?\n",
    "block_size = 8       # maximum sequence length considere for learning the model?\n",
    "n_embd = 32          # dimension of the embedding of the input tokens (also chosen as head size)\n",
    "n_head = 0           # number of heads\n",
    "n_layer = 1          # number of hidden layers (blocks)\n",
    "dropout = 0.0        # Regularization to avoid overfitting: drop out probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606e88f-1c4f-4817-833c-8ae86b4db40c",
   "metadata": {},
   "source": [
    "### Initialize parameters for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad7ab8-d49f-44f2-854c-63a6a7a7f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)              # Verify that GPU is used and the device is 'cuda' \n",
    "\n",
    "max_iters = 10000          # number of training iterations\n",
    "learning_rate = 1e-3       \n",
    "eval_interval = 500        # intervals at which the model performance is evaluated\n",
    "eval_iters = 200           # number of iterations used to estimate the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69531d21-db2a-4d69-9e99-73488906fcdf",
   "metadata": {},
   "source": [
    "### Set random seed\n",
    "\n",
    "Needed for reproducibility of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb6e50-fd3e-4a84-88f6-58d346c072d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fc18a-2376-462a-b5e9-be00351ae005",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Load raw text stored in the file `input.txt` containing  William Shakespeare's works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb65a7a-e4ea-4e81-a91a-a90541f4762a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070766fe-3837-4067-8721-5132953294c6",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Simple character based tokenizer: Encoding and decoding of characters to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bef804-7bb6-4d9f-ac51-fc9d1597e3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]           # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04dfb2b-a15f-47d2-91f3-3ee47be39154",
   "metadata": {},
   "source": [
    "### Generate training and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1dd505-7828-4c86-87e3-7db3a5ece0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))    # first 90% is training data, rest is validation data\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b3849-8f46-45c4-8813-57a971606ed5",
   "metadata": {},
   "source": [
    "### Define helper functions\n",
    "\n",
    "Select `batch_size` text sequences of length `block_size` at random locations in the text and store the data in a matrix of size `batch_size`x`block_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063cd21-18c5-4d74-b0f6-8243b9c2b10a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a874fc-3848-4249-8b5e-e3633320b332",
   "metadata": {},
   "source": [
    "Estimate the loss on both splits `train` and `val` by averaging the loss for `eval_iters` samples of data matrices generated by `get_batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc909aa4-2f4f-42ca-96c8-a4c809b472b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a865db4-d853-4054-a58e-bd29f9eb519f",
   "metadata": {},
   "source": [
    "### Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79351f06-a82d-4742-b8fe-2678fb01aee9",
   "metadata": {},
   "source": [
    "#### Self-attention model\n",
    "\n",
    "Core mechanism of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233da66-6b85-4f7a-9f4a-1e02a15d82b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.register_buffer('prob', torch.zeros(block_size, block_size)) \n",
    "        self.register_buffer('k', torch.zeros(block_size, head_size)) \n",
    "        self.register_buffer('q', torch.zeros(block_size, head_size)) \n",
    "        self.register_buffer('v', torch.zeros(block_size, head_size)) \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "\n",
    "        # store only first batch for visualization later during sequence generation\n",
    "        self.prob = wei[0,:,:]  \n",
    "        self.k = k[0,:,:]  \n",
    "        self.q = q[0,:,:]  \n",
    "        self.v = v[0,:,:] \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda0fea-8b2e-4f4b-be81-73d594be643a",
   "metadata": {},
   "source": [
    "Multiple heads are only used in the final simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8d932-647f-4413-9122-11ae6c9f51b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c7f75-26a4-47d6-8fb9-65535bb00591",
   "metadata": {},
   "source": [
    "#### Standard feed-forward layer of a neural network with ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9d969-8cd5-4812-bd4e-1520fd4612a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57848284-dd34-4dab-81b5-477e40dad731",
   "metadata": {},
   "source": [
    "#### Definition of a total layer of the neural network model \n",
    "Containing a standard feed-forward layer, skip-layers, self-attention and normalization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706a4af-cfc8-4590-84bb-85938eb9a75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        if n_head>0: \n",
    "            head_size = n_embd // n_head\n",
    "            self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if n_head>0:\n",
    "            x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4e90a-3993-4de3-9d25-47e395065e7c",
   "metadata": {},
   "source": [
    "#### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9870750-dba9-447a-a86b-9e547a913b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121df85-8c9f-485b-9f47-8442f27d8bc4",
   "metadata": {},
   "source": [
    "### Create a GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c673da-830e-4518-9c2b-3e6609fffe8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e22f2-3a4d-41c6-abc2-3df338fbf4dd",
   "metadata": {},
   "source": [
    "#### Define loss and optimizer\n",
    "For this classification problem we use the cross entropy error function\n",
    "$$J = -\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K y_{ik} \\ln(q_{ik})$$\n",
    "which is available in `pytorch` as `functional.cross_entropy` (see `GPTLanguageModel.forward`).\n",
    "\n",
    "As optimizer we suggest to use `AdamW` - stochastic gradient-based optimization method. See the [arxive paper](https://arxiv.org/abs/1711.05101) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6c8e2-ddfd-4070-bf7e-d1a3a452fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627ced8-0fd2-4efb-9ac0-f2c9bd884de2",
   "metadata": {},
   "source": [
    "### Run training of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f438d-b123-4a7e-b544-b688144e6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20542b-da65-48ec-bea7-ec19e4d81196",
   "metadata": {},
   "source": [
    "### Simulate the GPT model for text generation\n",
    "\n",
    "Define the initial text, i.e. the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b2681-1ed0-42a7-9a61-ea22271bf4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Lord:\n",
    "Hence comes it that your kindred shuns your house,\n",
    "As beaten hence by your strange lunacy.\n",
    "O noble lord, bethink thee of thy birth,\n",
    "Call home thy ancient thoughts from banishment\n",
    "And \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5f867-25b9-456a-867b-db91f550a090",
   "metadata": {},
   "source": [
    "Encode the text by the tokenizer, simulate the model for one step, decode and print the mode output.\n",
    "\n",
    "You can run the following cell multiple times to generate a longer text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95229bb6-cc86-4b9e-b61b-ab9be9e8a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.tensor( encode(context) , dtype=torch.long, device=device)\n",
    "context = decode(m.generate(context[None,:], max_new_tokens=1)[0].tolist())\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d50866-1b2c-4a98-a90d-f0bd23024c7a",
   "metadata": {},
   "source": [
    "### Explore the effect of self-attention on the performance\n",
    "\n",
    "- Run the neural network without self-attention by setting `n_head = 0` to obtain a reference performance value.\n",
    "- Run the neural network with self-attention and various values for `block_size = 2,4,8,16`.\n",
    "- How does the `block_size' influence the training loss and the quality of the text generation? Why?\n",
    "- Is there overfitting? Why or why not?\n",
    "- Investigate the self-attention mechanism visualized below. Find context examples where self-attention matters.\n",
    " \n",
    "- Set the network and learning parameters to the values below for the full GPT2 model and run a final text generation. Attention: 20 min learning time\n",
    "\n",
    "(Optional)\n",
    "- Run the full model with and without dropout and investigate the effect of the `dropout` propbability on the pe\n",
    "rformance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec1a17-8333-48d8-942f-b2a56738f48a",
   "metadata": {},
   "source": [
    "### Visualization of the self-attention mechanism \n",
    "(for `n_head = 1`)\n",
    "\n",
    "Get key, queries and values of the self-attention mechanism for the last time step of the simulation in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3e0b0-9e1c-41ac-85fd-491042618e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = m.blocks[0].sa.heads[0].prob.detach().cpu().numpy()\n",
    "k = m.blocks[0].sa.heads[0].k.detach().cpu().numpy()\n",
    "q = m.blocks[0].sa.heads[0].q.detach().cpu().numpy()\n",
    "v = m.blocks[0].sa.heads[0].v.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# k = m.blocks[0].sa.heads[0].key.weight.detach().cpu().numpy()\n",
    "# q = m.blocks[0].sa.heads[0].query.weight.detach().cpu().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "\n",
    "\n",
    "ax=axs[0,0]\n",
    "pi = ax.imshow(p)\n",
    "ax.set_title(context[-block_size-1:-1].replace('\\n','↵'))\n",
    "fig.colorbar(pi,label=\"Probability / attention score\") \n",
    "\n",
    "ax=axs[0,1]\n",
    "ax.imshow(k)\n",
    "ax.set_title('Key')\n",
    "\n",
    "ax=axs[1,0]\n",
    "ax.imshow(q)\n",
    "ax.set_title('Query')\n",
    "\n",
    "ax=axs[1,1]\n",
    "ax.set_axis_off()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b2476-f1fa-4441-8305-23925b94fb74",
   "metadata": {},
   "source": [
    "### Parameters for the final run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd9a41-90e3-4f1c-aa4a-2cc5c752943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "out = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31dd692-40e7-4943-a078-48830d52edeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch2",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
